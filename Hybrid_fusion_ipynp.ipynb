{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_fIVoYNEl5q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import parselmouth\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_DIR = \"/content/drive/MyDrive/Dataset/Cleaned_Vocals\"\n",
        "VISUAL_FEATURES_DIR = \"/content/drive/MyDrive/Dataset/Visual_Features\"\n",
        "SR = 16000\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 30\n",
        "LR = 1e-3\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "jZk1k1rpH_-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=SEED):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "def extract_id_and_label(filename: str):\n",
        "    match = re.search(r'(stress|nonstress)(\\d+)', filename, re.IGNORECASE)\n",
        "    if match:\n",
        "        label_str, file_id = match.group(1).lower(), match.group(2)\n",
        "        return file_id, label_str\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "n_EOXP5bH_2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio_features(audio_path: str) -> np.ndarray:\n",
        "    y, sr = librosa.load(audio_path, sr=SR)\n",
        "    snd = parselmouth.Sound(y, sr)\n",
        "\n",
        "    try:\n",
        "        pitch = snd.to_pitch()\n",
        "        f0 = pitch.selected_array[\"frequency\"]\n",
        "        f0_mean = float(np.mean(f0[f0 > 0])) if np.any(f0 > 0) else 0.0\n",
        "    except Exception:\n",
        "        f0_mean = 0.0\n",
        "\n",
        "    try:\n",
        "        point_process = parselmouth.praat.call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
        "        jitter = float(parselmouth.praat.call([snd, point_process], \"Get jitter (local)\", 0, 0, 75, 500, 1.3))\n",
        "        shimmer = float(parselmouth.praat.call([snd, point_process], \"Get shimmer (local)\", 0, 0, 75, 500, 1.3, 1.6))\n",
        "    except Exception:\n",
        "        jitter, shimmer = 0.0, 0.0\n",
        "\n",
        "    try:\n",
        "        hnr = parselmouth.praat.call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
        "        hnr_mean = float(parselmouth.praat.call(hnr, \"Get mean\", 0, 0))\n",
        "    except Exception:\n",
        "        hnr_mean = 0.0\n",
        "\n",
        "    try:\n",
        "        formant = snd.to_formant_burg()\n",
        "        f1 = float(parselmouth.praat.call(formant, \"Get mean\", 1, 0, 0))\n",
        "        f2 = float(parselmouth.praat.call(formant, \"Get mean\", 2, 0, 0))\n",
        "    except Exception:\n",
        "        f1, f2 = 0.0, 0.0\n",
        "\n",
        "    try:\n",
        "        energy = librosa.feature.rms(y=y)[0]\n",
        "        energy_thresh = 0.02\n",
        "        voiced = energy > energy_thresh\n",
        "        speaking_rate = float(np.sum(voiced) / max(1, len(voiced)) * (sr / 512))\n",
        "    except Exception:\n",
        "        speaking_rate = 0.0\n",
        "\n",
        "    return np.array([f0_mean, jitter, shimmer, hnr_mean, f1, f2, speaking_rate], dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "mLo-vuvwH_eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StressDataset(Dataset):\n",
        "    def __init__(self, audio_dir: str, visual_dir: str):\n",
        "        self.samples = []\n",
        "        self.visual_dim = None\n",
        "        self.logged_once = False\n",
        "\n",
        "        for subdir in [\"stress\", \"nonstress\"]:\n",
        "            subpath = os.path.join(audio_dir, subdir)\n",
        "            if not os.path.exists(subpath):\n",
        "                continue\n",
        "\n",
        "            for file in os.listdir(subpath):\n",
        "                if not file.endswith(\".wav\"):\n",
        "                    continue\n",
        "\n",
        "                file_id, label_str = extract_id_and_label(file)\n",
        "                if file_id is None:\n",
        "                    continue\n",
        "\n",
        "                vf_file = os.path.join(visual_dir, f\"{label_str}{file_id}_features.csv\")\n",
        "                if not os.path.exists(vf_file):\n",
        "                    continue\n",
        "\n",
        "                af = extract_audio_features(os.path.join(subpath, file))\n",
        "                vf_df = pd.read_csv(vf_file)\n",
        "                vf_df = vf_df.drop(columns=[\"timestamp\", \"video\", \"label\"], errors=\"ignore\")\n",
        "                vf_df = vf_df.select_dtypes(include=[np.number])\n",
        "\n",
        "                if not self.logged_once:\n",
        "                    print(f\"âœ… Using VF file: {vf_file}\")\n",
        "                    print(f\"   Kept {vf_df.shape[1]} numeric columns\")\n",
        "                    self.logged_once = True\n",
        "\n",
        "                if vf_df.empty:\n",
        "                    continue\n",
        "\n",
        "                if self.visual_dim is None:\n",
        "                    self.visual_dim = vf_df.shape[1]\n",
        "\n",
        "                vf_seq = vf_df.values.astype(np.float32)\n",
        "                length = vf_seq.shape[0]\n",
        "                label = 1 if label_str == \"stress\" else 0\n",
        "                self.samples.append((af, vf_seq, length, label))\n",
        "\n",
        "        if self.visual_dim is None:\n",
        "            self.visual_dim = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        af, vf_seq, length, label = self.samples[idx]\n",
        "        return (\n",
        "            torch.tensor(af, dtype=torch.float32),\n",
        "            torch.tensor(vf_seq, dtype=torch.float32),\n",
        "            torch.tensor(length, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )"
      ],
      "metadata": {
        "id": "pozjK2NkH_Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_collate(batch):\n",
        "    af_list, vf_list, len_list, label_list = zip(*batch)\n",
        "    af_batch = torch.stack(af_list, dim=0)\n",
        "    lengths = torch.stack(len_list, dim=0)\n",
        "    labels = torch.stack(label_list, dim=0)\n",
        "\n",
        "    T_max = max(v.shape[0] for v in vf_list)\n",
        "    Dv = vf_list[0].shape[1]\n",
        "    vf_padded = torch.zeros((len(vf_list), T_max, Dv), dtype=torch.float32)\n",
        "    for i, v in enumerate(vf_list):\n",
        "        vf_padded[i, :v.shape[0], :] = v\n",
        "\n",
        "    return af_batch, vf_padded, lengths, labels"
      ],
      "metadata": {
        "id": "owHU8dN7IaLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualTemporalCNN(nn.Module):\n",
        "    def __init__(self, visual_dim: int, hidden_out: int = 128):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=visual_dim, out_channels=256, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=hidden_out, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_out)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, vf_padded, lengths):\n",
        "        B, T, Dv = vf_padded.shape\n",
        "        if Dv == 0:\n",
        "            return torch.zeros((B, 128), device=vf_padded.device)\n",
        "        x = vf_padded.transpose(1, 2)\n",
        "        x = self.dropout(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.dropout(self.relu(self.bn2(self.conv2(x))))\n",
        "        mask = torch.arange(T, device=lengths.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
        "        mask = mask.unsqueeze(1)\n",
        "        x = x * mask\n",
        "        sums = x.sum(dim=2)\n",
        "        counts = mask.sum(dim=2).clamp(min=1)\n",
        "        return sums / counts\n"
      ],
      "metadata": {
        "id": "XWnXPZTsIaEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNZaNAjTI9vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, audio_dim: int, visual_dim: int, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.audio_fc = nn.Sequential(\n",
        "            nn.Linear(audio_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "        self.visual_temporal = VisualTemporalCNN(visual_dim=visual_dim, hidden_out=hidden)\n",
        "        self.feature_head = nn.Sequential(\n",
        "            nn.Linear(hidden * 2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden, 2)\n",
        "        )\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
        "        self.audio_head = nn.Linear(hidden, 2)\n",
        "        self.visual_head = nn.Linear(hidden, 2)\n",
        "\n",
        "    def forward(self, af, vf_padded, lengths):\n",
        "        a_h = self.audio_fc(af)\n",
        "        v_h = self.visual_temporal(vf_padded, lengths)\n",
        "        feat_logits = self.feature_head(torch.cat([a_h, v_h], dim=1))\n",
        "        a_logits = self.audio_head(a_h)\n",
        "        v_logits = self.visual_head(v_h)\n",
        "        dec_logits = self.alpha * a_logits + (1 - self.alpha) * v_logits\n",
        "        return (feat_logits + dec_logits) / 2.0\n",
        "\n",
        "\n",
        "\n",
        "def train_model(dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR):\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(\"Dataset is empty. Check paths.\")\n",
        "\n",
        "    idxs = list(range(len(dataset)))\n",
        "    train_idx, test_idx = train_test_split(idxs, test_size=0.2, random_state=SEED, shuffle=True,\n",
        "                                           stratify=[s[3] for s in dataset.samples])\n",
        "\n",
        "    train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx),\n",
        "                              batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "    test_loader = DataLoader(torch.utils.data.Subset(dataset, test_idx),\n",
        "                             batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n",
        "\n",
        "    model = FusionModel(audio_dim=7, visual_dim=dataset.visual_dim).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for af, vf_pad, lengths, labels in train_loader:\n",
        "            af, vf_pad, lengths, labels = af.to(DEVICE), vf_pad.to(DEVICE), lengths.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(af, vf_pad, lengths)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}/{epochs} - Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # ==== EVALUATE ====\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for af, vf_pad, lengths, labels in test_loader:\n",
        "            af, vf_pad, lengths = af.to(DEVICE), vf_pad.to(DEVICE), lengths.to(DEVICE)\n",
        "            logits = model(af, vf_pad, lengths)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            y_true.extend(labels.numpy().tolist())\n",
        "            y_pred.extend(preds.tolist())\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "\n",
        "    save_path = \"/content/drive/MyDrive/Stress_Fusion_Model.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"âœ… Model saved successfully at: {save_path}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = StressDataset(AUDIO_DIR, VISUAL_FEATURES_DIR)\n",
        "    print(f\"Loaded {len(dataset)} samples | Visual feature dim = {dataset.visual_dim}\")\n",
        "\n",
        "    model = train_model(dataset)\n",
        "\n",
        "\n",
        "    print(\"\\nðŸ” Loading saved model for testing...\")\n",
        "    loaded_model = FusionModel(audio_dim=7, visual_dim=dataset.visual_dim)\n",
        "    loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Stress_Fusion_Model.pth\"))\n",
        "    loaded_model.to(DEVICE)\n",
        "    loaded_model.eval()\n",
        "    print(\"âœ… Model loaded and ready for future testing!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO5di_N6-lkt",
        "outputId": "104d6974-3ec6-4a1e-b4fb-d649f55cd944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Using VF file: /content/drive/MyDrive/Dataset/Visual_Features/stress4_features.csv\n",
            "   Kept 2053 numeric columns\n",
            "Loaded 807 samples | Visual feature dim = 2053\n",
            "Epoch 1/30 - Loss: 2.8553\n",
            "Epoch 2/30 - Loss: 1.3764\n",
            "Epoch 3/30 - Loss: 0.8788\n",
            "Epoch 4/30 - Loss: 0.6320\n",
            "Epoch 5/30 - Loss: 0.5404\n",
            "Epoch 6/30 - Loss: 0.3722\n",
            "Epoch 7/30 - Loss: 0.3562\n",
            "Epoch 8/30 - Loss: 0.2226\n",
            "Epoch 9/30 - Loss: 0.3353\n",
            "Epoch 10/30 - Loss: 0.2430\n",
            "Epoch 11/30 - Loss: 0.1758\n",
            "Epoch 12/30 - Loss: 0.1966\n",
            "Epoch 13/30 - Loss: 0.1165\n",
            "Epoch 14/30 - Loss: 0.1876\n",
            "Epoch 15/30 - Loss: 0.1405\n",
            "Epoch 16/30 - Loss: 0.1392\n",
            "Epoch 17/30 - Loss: 0.0728\n",
            "Epoch 18/30 - Loss: 0.1124\n",
            "Epoch 19/30 - Loss: 0.0659\n",
            "Epoch 20/30 - Loss: 0.0951\n",
            "Epoch 21/30 - Loss: 0.0676\n",
            "Epoch 22/30 - Loss: 0.1431\n",
            "Epoch 23/30 - Loss: 0.1231\n",
            "Epoch 24/30 - Loss: 0.1019\n",
            "Epoch 25/30 - Loss: 0.1368\n",
            "Epoch 26/30 - Loss: 0.0619\n",
            "Epoch 27/30 - Loss: 0.1045\n",
            "Epoch 28/30 - Loss: 0.1632\n",
            "Epoch 29/30 - Loss: 0.0746\n",
            "Epoch 30/30 - Loss: 0.0331\n",
            "Accuracy: 0.7901234567901234\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7558    0.8333    0.7927        78\n",
            "           1     0.8289    0.7500    0.7875        84\n",
            "\n",
            "    accuracy                         0.7901       162\n",
            "   macro avg     0.7924    0.7917    0.7901       162\n",
            "weighted avg     0.7937    0.7901    0.7900       162\n",
            "\n",
            "âœ… Model saved successfully at: /content/drive/MyDrive/Stress_Fusion_Model.pth\n",
            "\n",
            "ðŸ” Loading saved model for testing...\n",
            "âœ… Model loaded and ready for future testing!\n"
          ]
        }
      ]
    }
  ]
}