{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osDWgRJX5dDT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_DIRS = {\n",
        "    \"stress\": \"/content/drive/MyDrive/Dataset/Cleaned_Vocals/stress\",\n",
        "    \"nonstress\": \"/content/drive/MyDrive/Dataset/Cleaned_Vocals/nonstress\"\n",
        "}\n",
        "VISUAL_DIR = \"/content/drive/MyDrive/Dataset/Visual_Features\"\n",
        "\n",
        "MAX_LEN = 200\n",
        "SAMPLE_RATE = 16000\n",
        "N_MFCC = 40\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==============================\n",
        "# Utilities\n",
        "# ==============================\n",
        "def extract_digits(name: str):\n",
        "    return ''.join(filter(str.isdigit, name))\n",
        "\n",
        "def resample_sequence(seq, target_len):\n",
        "    T, D = seq.shape\n",
        "    x_old = np.linspace(0, 1, T)\n",
        "    x_new = np.linspace(0, 1, target_len)\n",
        "    seq_resampled = np.zeros((target_len, D))\n",
        "    for d in range(D):\n",
        "        seq_resampled[:, d] = np.interp(x_new, x_old, seq[:, d])\n",
        "    return seq_resampled.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "lBNlsdrR6gnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio_features(audio_path, max_len=MAX_LEN, target_len=None):\n",
        "    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC).T  # [T, 40]\n",
        "\n",
        "    if target_len is not None:\n",
        "        mfcc = resample_sequence(mfcc, target_len)\n",
        "\n",
        "    if mfcc.shape[0] < max_len:\n",
        "        mfcc = np.pad(mfcc, ((0, max_len - mfcc.shape[0]), (0, 0)))\n",
        "    else:\n",
        "        mfcc = mfcc[:max_len, :]\n",
        "\n",
        "    return mfcc.astype(np.float32)\n",
        "\n",
        "def load_visual_features(visual_path, max_len=MAX_LEN):\n",
        "    df = pd.read_csv(visual_path, on_bad_lines=\"skip\", encoding=\"ISO-8859-1\")\n",
        "    df = df.select_dtypes(include=[np.number])\n",
        "    feats = df.values\n",
        "\n",
        "    if feats.shape[0] < max_len:\n",
        "        feats = np.pad(feats, ((0, max_len - feats.shape[0]), (0, 0)))\n",
        "    else:\n",
        "        feats = feats[:max_len, :]\n",
        "\n",
        "    return feats.astype(np.float32)"
      ],
      "metadata": {
        "id": "Sfzf2cYv6gj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, audio_dirs, visual_dir):\n",
        "        self.pairs = []\n",
        "        all_visual_files = [f for f in os.listdir(visual_dir) if f.endswith(\".csv\")]\n",
        "\n",
        "        audio_files = []\n",
        "        for label, folder in audio_dirs.items():\n",
        "            for f in os.listdir(folder):\n",
        "                if f.endswith(\".wav\"):\n",
        "                    audio_files.append((os.path.join(folder, f), label))\n",
        "\n",
        "        for af, label in audio_files:\n",
        "            base = os.path.splitext(os.path.basename(af))[0].lower()\n",
        "            base_digits = extract_digits(base)\n",
        "            vf_candidates = [vf for vf in all_visual_files if base_digits in vf]\n",
        "\n",
        "            if vf_candidates:\n",
        "                vf = os.path.join(visual_dir, vf_candidates[0])\n",
        "                self.pairs.append((af, vf, 1 if label == \"stress\" else 0))\n",
        "\n",
        "        print(f\"âœ… Found {len(self.pairs)} matching pairs\")\n",
        "\n",
        "        # detect max visual dim\n",
        "        max_dim = 0\n",
        "        for _, vpath, _ in self.pairs:\n",
        "            df = pd.read_csv(vpath, on_bad_lines=\"skip\", encoding=\"ISO-8859-1\")\n",
        "            df = df.select_dtypes(include=[np.number])\n",
        "            max_dim = max(max_dim, df.shape[1])\n",
        "        self.visual_dim = max_dim\n",
        "        print(f\"ðŸ“Š Standardizing visual features to {self.visual_dim} dims\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        af, vf, label = self.pairs[idx]\n",
        "\n",
        "        visual = load_visual_features(vf, max_len=MAX_LEN)\n",
        "        target_len = visual.shape[0]\n",
        "        audio = load_audio_features(af, max_len=MAX_LEN, target_len=target_len)\n",
        "\n",
        "        if visual.shape[1] < self.visual_dim:\n",
        "            visual = np.pad(visual, ((0, 0), (0, self.visual_dim - visual.shape[1])))\n",
        "        elif visual.shape[1] > self.visual_dim:\n",
        "            visual = visual[:, :self.visual_dim]\n",
        "\n",
        "        return torch.tensor(audio), torch.tensor(visual), torch.tensor(label)"
      ],
      "metadata": {
        "id": "zhgb5WZP6ghL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LvzEccsO7W79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AudioVisualFusionModel(nn.Module):\n",
        "    def __init__(self, audio_dim=40, video_dim=2054, d_model=256, nhead=4, num_layers=2, num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Project audio & video to same dimension\n",
        "        self.audio_proj = nn.Linear(audio_dim, d_model)\n",
        "        self.video_proj = nn.Linear(video_dim, d_model)\n",
        "\n",
        "        # Per-time-step fusion [A_t || V_t]\n",
        "        self.fuse_proj = nn.Linear(2 * d_model, d_model)\n",
        "\n",
        "        # Learnable [CLS] token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "        # Transformer encoder for fused sequence\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.fusion_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Bi-directional cross-attention\n",
        "        self.cross_attn_audio_to_video = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
        "        self.cross_attn_video_to_audio = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Final transformer after cross-attention\n",
        "        fusion_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.fusion_transformer = nn.TransformerEncoder(fusion_layer, num_layers=2)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio, video):\n",
        "        # Project features\n",
        "        audio = self.audio_proj(audio)\n",
        "        video = self.video_proj(video)\n",
        "\n",
        "        # Per-time-step fusion\n",
        "        fused = torch.cat([audio, video], dim=-1)   # [B, T, 2*d_model]\n",
        "        fused = self.fuse_proj(fused)               # [B, T, d_model]\n",
        "\n",
        "        # Add [CLS] token\n",
        "        B = fused.size(0)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        fused_seq = torch.cat([cls_tokens, fused], dim=1)\n",
        "\n",
        "        # Transformer encoder\n",
        "        enc = self.fusion_encoder(fused_seq)\n",
        "\n",
        "        # Cross-attention (bi-directional)\n",
        "        enc_audio = enc[:, 1:, :]  # remove CLS\n",
        "        enc_video = enc[:, 1:, :]\n",
        "        a2v, _ = self.cross_attn_audio_to_video(enc_audio, enc_video, enc_video)\n",
        "        v2a, _ = self.cross_attn_video_to_audio(enc_video, enc_audio, enc_audio)\n",
        "\n",
        "        # Re-fuse\n",
        "        fused_seq = torch.cat([cls_tokens, a2v + v2a], dim=1)\n",
        "\n",
        "        # Fusion transformer\n",
        "        fused_out = self.fusion_transformer(fused_seq)\n",
        "\n",
        "        # CLS token for classification\n",
        "        cls_out = fused_out[:, 0, :]\n",
        "        logits = self.classifier(cls_out)\n",
        "        return logits\n",
        "\n",
        "# ==============================\n",
        "# Training + Testing\n",
        "# ==============================\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio, visual, labels in loader:\n",
        "            audio, visual, labels = audio.to(device), visual.to(device), labels.to(device)\n",
        "            outputs = model(audio.float(), visual.float())\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "def train_model(audio_dirs, visual_dir, epochs=30, save_dir=\"/content/models\"):\n",
        "    dataset = MultimodalDataset(audio_dirs, visual_dir)\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(\"âŒ No matching audio-visual pairs found!\")\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Split\n",
        "    train_idx, temp_idx = train_test_split(range(len(dataset)), test_size=0.4, random_state=42)\n",
        "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx), batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(torch.utils.data.Subset(dataset, val_idx), batch_size=8)\n",
        "    test_loader = DataLoader(torch.utils.data.Subset(dataset, test_idx), batch_size=8)\n",
        "\n",
        "    sample_audio, sample_visual, _ = dataset[0]\n",
        "    model = AudioVisualFusionModel(audio_dim=sample_audio.shape[1], video_dim=sample_visual.shape[1]).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    best_val_f1 = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for audio, visual, labels in train_loader:\n",
        "            audio, visual, labels = audio.to(device), visual.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(audio.float(), visual.float())\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        val_acc, val_prec, val_rec, val_f1 = evaluate_model(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f} \"\n",
        "              f\"| Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f} \"\n",
        "              f\"| Prec: {val_prec:.2f} | Recall: {val_rec:.2f} | F1: {val_f1:.2f}\")\n",
        "\n",
        "        # Save best model by validation F1\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/best_model.pt\")\n",
        "\n",
        "    # âœ… Final Testing\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/best_model.pt\"))\n",
        "    test_acc, test_prec, test_rec, test_f1 = evaluate_model(model, test_loader)\n",
        "    print(\"\\nðŸŽ¯ Final Test Results:\")\n",
        "    print(f\"Accuracy: {test_acc:.2f} | Precision: {test_prec:.2f} | Recall: {test_rec:.2f} | F1: {test_f1:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ==============================\n",
        "# Run\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    model = train_model(AUDIO_DIRS, VISUAL_DIR, epochs=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSYnWwPEtdb8",
        "outputId": "270a4e81-c958-4e12-cf28-ad3aadbe87d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found 814 matching pairs\n",
            "ðŸ“Š Standardizing visual features to 2054 dims\n",
            "Epoch 1/30 | Loss: 0.7179 | Train Acc: 48.36% | Val Acc: 0.47 | Prec: 0.47 | Recall: 1.00 | F1: 0.64\n",
            "Epoch 2/30 | Loss: 0.6961 | Train Acc: 52.46% | Val Acc: 0.53 | Prec: 0.00 | Recall: 0.00 | F1: 0.00\n",
            "Epoch 3/30 | Loss: 0.7007 | Train Acc: 51.43% | Val Acc: 0.47 | Prec: 0.47 | Recall: 1.00 | F1: 0.64\n",
            "Epoch 4/30 | Loss: 0.6961 | Train Acc: 52.25% | Val Acc: 0.53 | Prec: 0.00 | Recall: 0.00 | F1: 0.00\n",
            "Epoch 5/30 | Loss: 0.6981 | Train Acc: 51.64% | Val Acc: 0.53 | Prec: 0.00 | Recall: 0.00 | F1: 0.00\n",
            "Epoch 6/30 | Loss: 0.7011 | Train Acc: 52.66% | Val Acc: 0.53 | Prec: 0.00 | Recall: 0.00 | F1: 0.00\n",
            "Epoch 7/30 | Loss: 0.6926 | Train Acc: 53.48% | Val Acc: 0.53 | Prec: 0.00 | Recall: 0.00 | F1: 0.00\n",
            "Epoch 8/30 | Loss: 0.6695 | Train Acc: 59.63% | Val Acc: 0.62 | Prec: 0.79 | Recall: 0.25 | F1: 0.38\n",
            "Epoch 9/30 | Loss: 0.6462 | Train Acc: 64.14% | Val Acc: 0.64 | Prec: 0.64 | Recall: 0.51 | F1: 0.57\n",
            "Epoch 10/30 | Loss: 0.6407 | Train Acc: 63.93% | Val Acc: 0.60 | Prec: 0.54 | Recall: 0.99 | F1: 0.70\n",
            "Epoch 11/30 | Loss: 0.6058 | Train Acc: 67.42% | Val Acc: 0.66 | Prec: 0.59 | Recall: 0.95 | F1: 0.72\n",
            "Epoch 12/30 | Loss: 0.5845 | Train Acc: 71.52% | Val Acc: 0.55 | Prec: 0.51 | Recall: 0.99 | F1: 0.67\n",
            "Epoch 13/30 | Loss: 0.5549 | Train Acc: 70.29% | Val Acc: 0.74 | Prec: 0.68 | Recall: 0.83 | F1: 0.75\n",
            "Epoch 14/30 | Loss: 0.5268 | Train Acc: 73.77% | Val Acc: 0.66 | Prec: 0.89 | Recall: 0.32 | F1: 0.47\n",
            "Epoch 15/30 | Loss: 0.5473 | Train Acc: 73.16% | Val Acc: 0.74 | Prec: 0.80 | Recall: 0.58 | F1: 0.67\n",
            "Epoch 16/30 | Loss: 0.5176 | Train Acc: 76.23% | Val Acc: 0.72 | Prec: 0.88 | Recall: 0.47 | F1: 0.62\n",
            "Epoch 17/30 | Loss: 0.6021 | Train Acc: 71.52% | Val Acc: 0.66 | Prec: 0.95 | Recall: 0.28 | F1: 0.43\n",
            "Epoch 18/30 | Loss: 0.5126 | Train Acc: 78.48% | Val Acc: 0.70 | Prec: 0.66 | Recall: 0.75 | F1: 0.70\n",
            "Epoch 19/30 | Loss: 0.5402 | Train Acc: 76.64% | Val Acc: 0.69 | Prec: 0.80 | Recall: 0.46 | F1: 0.58\n",
            "Epoch 20/30 | Loss: 0.4682 | Train Acc: 79.51% | Val Acc: 0.71 | Prec: 0.77 | Recall: 0.54 | F1: 0.64\n",
            "Epoch 21/30 | Loss: 0.4656 | Train Acc: 78.28% | Val Acc: 0.74 | Prec: 0.74 | Recall: 0.70 | F1: 0.72\n",
            "Epoch 22/30 | Loss: 0.4790 | Train Acc: 78.89% | Val Acc: 0.63 | Prec: 0.56 | Recall: 0.97 | F1: 0.71\n",
            "Epoch 23/30 | Loss: 0.4817 | Train Acc: 76.84% | Val Acc: 0.70 | Prec: 0.70 | Recall: 0.62 | F1: 0.66\n",
            "Epoch 24/30 | Loss: 0.4659 | Train Acc: 80.12% | Val Acc: 0.69 | Prec: 0.62 | Recall: 0.88 | F1: 0.73\n",
            "Epoch 25/30 | Loss: 0.4367 | Train Acc: 82.17% | Val Acc: 0.75 | Prec: 0.73 | Recall: 0.74 | F1: 0.73\n",
            "Epoch 26/30 | Loss: 0.4851 | Train Acc: 78.89% | Val Acc: 0.71 | Prec: 0.68 | Recall: 0.72 | F1: 0.70\n",
            "Epoch 27/30 | Loss: 0.4249 | Train Acc: 82.38% | Val Acc: 0.68 | Prec: 0.66 | Recall: 0.64 | F1: 0.65\n",
            "Epoch 28/30 | Loss: 0.4298 | Train Acc: 82.79% | Val Acc: 0.70 | Prec: 0.74 | Recall: 0.55 | F1: 0.63\n",
            "Epoch 29/30 | Loss: 0.4481 | Train Acc: 81.56% | Val Acc: 0.69 | Prec: 0.88 | Recall: 0.38 | F1: 0.53\n",
            "Epoch 30/30 | Loss: 0.4393 | Train Acc: 83.20% | Val Acc: 0.69 | Prec: 0.71 | Recall: 0.58 | F1: 0.64\n",
            "\n",
            "ðŸŽ¯ Final Test Results:\n",
            "Accuracy: 0.77 | Precision: 0.72 | Recall: 0.86 | F1: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5EQyyVG7WyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}