{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85Ctq0H-GVgl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_DIRS = {\n",
        "    \"stress\": \"/content/drive/MyDrive/Dataset/Cleaned_Vocals/stress\",\n",
        "    \"nonstress\": \"/content/drive/MyDrive/Dataset/Cleaned_Vocals/nonstress\"\n",
        "}\n",
        "VISUAL_DIR = \"/content/drive/MyDrive/Dataset/Visual_Features\"\n",
        "\n",
        "MAX_LEN = 200\n",
        "SAMPLE_RATE = 16000\n",
        "N_MFCC = 40\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==============================\n",
        "# Utilities\n",
        "# ==============================\n",
        "def extract_digits(name: str):\n",
        "    return ''.join(filter(str.isdigit, name))\n",
        "\n",
        "def resample_sequence(seq, target_len):\n",
        "    \"\"\"Resample a sequence (T, D) to new length target_len.\"\"\"\n",
        "    T, D = seq.shape\n",
        "    x_old = np.linspace(0, 1, T)\n",
        "    x_new = np.linspace(0, 1, target_len)\n",
        "    seq_resampled = np.zeros((target_len, D))\n",
        "    for d in range(D):\n",
        "        seq_resampled[:, d] = np.interp(x_new, x_old, seq[:, d])\n",
        "    return seq_resampled.astype(np.float32)\n",
        "\n",
        "# ==============================\n",
        "# Feature loaders\n",
        "# ==============================\n",
        "def load_audio_features(audio_path, max_len=MAX_LEN, target_len=None):\n",
        "    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC).T  # [T, 40]\n",
        "\n",
        "    if target_len is not None:\n",
        "        mfcc = resample_sequence(mfcc, target_len)\n",
        "\n",
        "    if mfcc.shape[0] < max_len:\n",
        "        mfcc = np.pad(mfcc, ((0, max_len - mfcc.shape[0]), (0, 0)))\n",
        "    else:\n",
        "        mfcc = mfcc[:max_len, :]\n",
        "\n",
        "    return mfcc.astype(np.float32)\n",
        "\n",
        "def load_visual_features(visual_path, max_len=MAX_LEN):\n",
        "    df = pd.read_csv(visual_path, on_bad_lines=\"skip\", encoding=\"ISO-8859-1\")\n",
        "    df = df.select_dtypes(include=[np.number])\n",
        "    feats = df.values\n",
        "\n",
        "    if feats.shape[0] < max_len:\n",
        "        feats = np.pad(feats, ((0, max_len - feats.shape[0]), (0, 0)))\n",
        "    else:\n",
        "        feats = feats[:max_len, :]\n",
        "\n",
        "    return feats.astype(np.float32)\n",
        "\n",
        "# ==============================\n",
        "# Dataset\n",
        "# ==============================\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, audio_dirs, visual_dir):\n",
        "        self.pairs = []\n",
        "        all_visual_files = [f for f in os.listdir(visual_dir) if f.endswith(\".csv\")]\n",
        "\n",
        "        audio_files = []\n",
        "        for label, folder in audio_dirs.items():\n",
        "            for f in os.listdir(folder):\n",
        "                if f.endswith(\".wav\"):\n",
        "                    audio_files.append((os.path.join(folder, f), label))\n",
        "\n",
        "        for af, label in audio_files:\n",
        "            base = os.path.splitext(os.path.basename(af))[0].lower()\n",
        "            base_digits = extract_digits(base)\n",
        "            vf_candidates = [vf for vf in all_visual_files if base_digits in vf]\n",
        "\n",
        "            if vf_candidates:\n",
        "                vf = os.path.join(visual_dir, vf_candidates[0])\n",
        "                self.pairs.append((af, vf, 1 if label == \"stress\" else 0))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.pairs)} matching pairs\")\n",
        "\n",
        "        # detect max visual dim\n",
        "        max_dim = 0\n",
        "        for _, vpath, _ in self.pairs:\n",
        "            df = pd.read_csv(vpath, on_bad_lines=\"skip\", encoding=\"ISO-8859-1\")\n",
        "            df = df.select_dtypes(include=[np.number])\n",
        "            max_dim = max(max_dim, df.shape[1])\n",
        "        self.visual_dim = max_dim\n",
        "        print(f\"üìä Standardizing visual features to {self.visual_dim} dims\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        af, vf, label = self.pairs[idx]\n",
        "\n",
        "        visual = load_visual_features(vf, max_len=MAX_LEN)\n",
        "        target_len = visual.shape[0]\n",
        "        audio = load_audio_features(af, max_len=MAX_LEN, target_len=target_len)\n",
        "\n",
        "        if visual.shape[1] < self.visual_dim:\n",
        "            visual = np.pad(visual, ((0, 0), (0, self.visual_dim - visual.shape[1])))\n",
        "        elif visual.shape[1] > self.visual_dim:\n",
        "            visual = visual[:, :self.visual_dim]\n",
        "\n",
        "        return torch.tensor(audio), torch.tensor(visual), torch.tensor(label)"
      ],
      "metadata": {
        "id": "PxNd_jZTaZWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WG2231a-aTU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================\n",
        "# Audio-Visual Fusion Model\n",
        "# ==============================\n",
        "class AudioVisualFusionModel(nn.Module):\n",
        "    def __init__(self, audio_dim=40, video_dim=2054, d_model=256, nhead=4, num_layers=2, num_classes=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Project audio & video features to the same dimension\n",
        "        self.audio_proj = nn.Linear(audio_dim, d_model)\n",
        "        self.video_proj = nn.Linear(video_dim, d_model)\n",
        "\n",
        "        # LayerNorm for stability\n",
        "        self.audio_norm = nn.LayerNorm(d_model)\n",
        "        self.video_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Learnable [CLS] token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "        # Transformer encoders for audio & video\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.audio_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.video_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Cross-attention blocks\n",
        "        self.cross_attn_audio_to_video = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "        self.cross_attn_video_to_audio = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "\n",
        "        # Fusion transformer (operates on concatenated sequence with CLS)\n",
        "        fusion_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.fusion_transformer = nn.TransformerEncoder(fusion_layer, num_layers=2)\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio, video):\n",
        "        # Project & normalize inputs\n",
        "        audio = self.audio_norm(self.audio_proj(audio))\n",
        "        video = self.video_norm(self.video_proj(video))\n",
        "\n",
        "        # Encode separately\n",
        "        a_enc = self.audio_encoder(audio)\n",
        "        v_enc = self.video_encoder(video)\n",
        "\n",
        "        # Cross-attention (bi-directional)\n",
        "        a2v, _ = self.cross_attn_audio_to_video(a_enc, v_enc, v_enc)\n",
        "        v2a, _ = self.cross_attn_video_to_audio(v_enc, a_enc, a_enc)\n",
        "\n",
        "        # Fuse cross-attended features\n",
        "        fused_seq = torch.cat([a2v, v2a], dim=1)\n",
        "\n",
        "        # Add [CLS] token at start\n",
        "        B = fused_seq.size(0)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        fused_seq = torch.cat([cls_tokens, fused_seq], dim=1)\n",
        "\n",
        "        # Fusion transformer\n",
        "        fused_out = self.fusion_transformer(fused_seq)\n",
        "\n",
        "        # Take CLS token output\n",
        "        cls_out = fused_out[:, 0, :]\n",
        "\n",
        "        # Classify\n",
        "        logits = self.classifier(cls_out)\n",
        "        return logits\n",
        "\n",
        "# ==============================\n",
        "# Training + Testing\n",
        "# ==============================\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for audio, visual, labels in loader:\n",
        "            audio, visual, labels = audio.to(device), visual.to(device), labels.to(device)\n",
        "            outputs = model(audio.float(), visual.float())\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "def train_model(audio_dirs, visual_dir, epochs=30, save_dir=\"/content/models\"):\n",
        "    dataset = MultimodalDataset(audio_dirs, visual_dir)\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(\"‚ùå No matching audio-visual pairs found!\")\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 60% train, 20% val, 20% test\n",
        "    train_idx, temp_idx = train_test_split(range(len(dataset)), test_size=0.4, random_state=42)\n",
        "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx), batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(torch.utils.data.Subset(dataset, val_idx), batch_size=8)\n",
        "    test_loader = DataLoader(torch.utils.data.Subset(dataset, test_idx), batch_size=8)\n",
        "\n",
        "    sample_audio, sample_visual, _ = dataset[0]\n",
        "    model = AudioVisualFusionModel(audio_dim=sample_audio.shape[1], video_dim=sample_visual.shape[1]).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for audio, visual, labels in train_loader:\n",
        "            audio, visual, labels = audio.to(device), visual.to(device), labels.to(device)\n",
        "\n",
        "            # Debug check for NaNs\n",
        "            if torch.isnan(audio).any() or torch.isnan(visual).any():\n",
        "                raise ValueError(\"NaN detected in input features!\")\n",
        "\n",
        "            outputs = model(audio.float(), visual.float())\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        val_acc, val_prec, val_rec, val_f1 = evaluate_model(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f} \"\n",
        "              f\"| Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f} \"\n",
        "              f\"| Prec: {val_prec:.2f} | Recall: {val_rec:.2f} | F1: {val_f1:.2f}\")\n",
        "\n",
        "        # save checkpoint\n",
        "        torch.save(model.state_dict(), f\"{save_dir}/model_epoch{epoch+1}.pt\")\n",
        "\n",
        "    # ‚úÖ Final Testing\n",
        "    test_acc, test_prec, test_rec, test_f1 = evaluate_model(model, test_loader)\n",
        "    print(\"\\nüéØ Final Test Results:\")\n",
        "    print(f\"Accuracy: {test_acc:.2f} | Precision: {test_prec:.2f} | Recall: {test_rec:.2f} | F1: {test_f1:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ==============================\n",
        "# Run\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    model = train_model(AUDIO_DIRS, VISUAL_DIR, epochs=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ4E576OhNq7",
        "outputId": "0a21e5ea-2914-49af-8c4b-9dc28d6f39e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Found 814 matching pairs\n",
            "üìä Standardizing visual features to 2054 dims\n",
            "Epoch 1/30 | Loss: 0.7072 | Train Acc: 52.05% | Val Acc: 0.53 | Prec: 0.00 | Recall: 0.00 | F1: 0.00\n",
            "Epoch 2/30 | Loss: 0.7014 | Train Acc: 52.46% | Val Acc: 0.53 | Prec: 0.00 | Recall: 0.00 | F1: 0.00\n",
            "Epoch 3/30 | Loss: 0.7030 | Train Acc: 54.10% | Val Acc: 0.55 | Prec: 0.51 | Recall: 0.79 | F1: 0.62\n",
            "Epoch 4/30 | Loss: 0.6865 | Train Acc: 57.58% | Val Acc: 0.61 | Prec: 0.84 | Recall: 0.21 | F1: 0.34\n",
            "Epoch 5/30 | Loss: 0.6000 | Train Acc: 67.62% | Val Acc: 0.74 | Prec: 0.75 | Recall: 0.66 | F1: 0.70\n",
            "Epoch 6/30 | Loss: 0.5771 | Train Acc: 72.95% | Val Acc: 0.71 | Prec: 0.97 | Recall: 0.38 | F1: 0.55\n",
            "Epoch 7/30 | Loss: 0.5183 | Train Acc: 78.28% | Val Acc: 0.74 | Prec: 0.74 | Recall: 0.70 | F1: 0.72\n",
            "Epoch 8/30 | Loss: 0.4882 | Train Acc: 78.07% | Val Acc: 0.75 | Prec: 0.93 | Recall: 0.51 | F1: 0.66\n",
            "Epoch 9/30 | Loss: 0.4977 | Train Acc: 79.30% | Val Acc: 0.72 | Prec: 0.97 | Recall: 0.41 | F1: 0.57\n",
            "Epoch 10/30 | Loss: 0.4480 | Train Acc: 82.17% | Val Acc: 0.75 | Prec: 0.89 | Recall: 0.53 | F1: 0.66\n",
            "Epoch 11/30 | Loss: 0.5007 | Train Acc: 80.33% | Val Acc: 0.73 | Prec: 0.66 | Recall: 0.86 | F1: 0.75\n",
            "Epoch 12/30 | Loss: 0.3625 | Train Acc: 87.09% | Val Acc: 0.74 | Prec: 0.71 | Recall: 0.76 | F1: 0.73\n",
            "Epoch 13/30 | Loss: 0.3664 | Train Acc: 86.68% | Val Acc: 0.75 | Prec: 0.72 | Recall: 0.76 | F1: 0.74\n",
            "Epoch 14/30 | Loss: 0.3312 | Train Acc: 88.93% | Val Acc: 0.74 | Prec: 0.79 | Recall: 0.61 | F1: 0.69\n",
            "Epoch 15/30 | Loss: 0.3401 | Train Acc: 88.11% | Val Acc: 0.74 | Prec: 0.79 | Recall: 0.59 | F1: 0.68\n",
            "Epoch 16/30 | Loss: 0.3231 | Train Acc: 89.96% | Val Acc: 0.74 | Prec: 0.69 | Recall: 0.80 | F1: 0.74\n",
            "Epoch 17/30 | Loss: 0.3130 | Train Acc: 89.96% | Val Acc: 0.74 | Prec: 0.81 | Recall: 0.57 | F1: 0.67\n",
            "Epoch 18/30 | Loss: 0.3303 | Train Acc: 90.37% | Val Acc: 0.74 | Prec: 0.68 | Recall: 0.82 | F1: 0.74\n",
            "Epoch 19/30 | Loss: 0.3070 | Train Acc: 90.78% | Val Acc: 0.73 | Prec: 0.81 | Recall: 0.55 | F1: 0.66\n",
            "Epoch 20/30 | Loss: 0.2371 | Train Acc: 92.42% | Val Acc: 0.71 | Prec: 0.64 | Recall: 0.86 | F1: 0.73\n",
            "Epoch 21/30 | Loss: 0.2079 | Train Acc: 93.65% | Val Acc: 0.73 | Prec: 0.79 | Recall: 0.58 | F1: 0.67\n",
            "Epoch 22/30 | Loss: 0.1795 | Train Acc: 95.08% | Val Acc: 0.73 | Prec: 0.90 | Recall: 0.47 | F1: 0.62\n",
            "Epoch 23/30 | Loss: 0.3760 | Train Acc: 89.75% | Val Acc: 0.73 | Prec: 0.73 | Recall: 0.67 | F1: 0.70\n",
            "Epoch 24/30 | Loss: 0.2026 | Train Acc: 94.88% | Val Acc: 0.77 | Prec: 0.80 | Recall: 0.67 | F1: 0.73\n",
            "Epoch 25/30 | Loss: 0.2065 | Train Acc: 93.85% | Val Acc: 0.74 | Prec: 0.72 | Recall: 0.72 | F1: 0.72\n",
            "Epoch 26/30 | Loss: 0.1606 | Train Acc: 95.70% | Val Acc: 0.73 | Prec: 0.85 | Recall: 0.51 | F1: 0.64\n",
            "Epoch 27/30 | Loss: 0.2213 | Train Acc: 94.67% | Val Acc: 0.72 | Prec: 0.70 | Recall: 0.68 | F1: 0.69\n",
            "Epoch 28/30 | Loss: 0.1596 | Train Acc: 96.31% | Val Acc: 0.77 | Prec: 0.87 | Recall: 0.59 | F1: 0.70\n",
            "Epoch 29/30 | Loss: 0.1061 | Train Acc: 97.34% | Val Acc: 0.76 | Prec: 0.78 | Recall: 0.68 | F1: 0.73\n",
            "Epoch 30/30 | Loss: 0.1369 | Train Acc: 96.52% | Val Acc: 0.77 | Prec: 0.82 | Recall: 0.64 | F1: 0.72\n",
            "\n",
            "üéØ Final Test Results:\n",
            "Accuracy: 0.71 | Precision: 0.73 | Recall: 0.62 | F1: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAljMlvaaTRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbPQiQOTaTKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}